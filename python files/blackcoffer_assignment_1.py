# -*- coding: utf-8 -*-
"""BlackCoffer_Assignment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NREj4rQtpqr01iebON9O6ex4g1iOiF1_
"""

from google.colab import drive
drive.mount('/gdrive')

pip install beautifulsoup4

pip install nltk

pip install requests

import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.corpus import cmudict
from openpyxl import load_workbook

nltk.download('punkt')
nltk.download('cmudict')
import re

"""#extracting stopwords from the given stopwords file in to all_stopwords list"""

stopwords_directory = '/gdrive/My Drive/StopWords'
# List to store stopwords from all files
all_stopwords = []
# Iterate over each file in the directory
for filename in os.listdir(stopwords_directory):
    filepath = os.path.join(stopwords_directory, filename)

    # Check if the file is a text file
    if filename.endswith(".txt"):
        # Read stopwords from the file
        with open(filepath, 'r', encoding='ISO-8859-1') as file:
            stopwords_in_file = [line.strip() for line in file.readlines()]
            all_stopwords.extend(stopwords_in_file)

"""# Extracting all positive & negative words from given MasterDictionary to positive_words,negative_words list"""

positive_words = []
negative_words = []

positive_path = '/gdrive/My Drive/MasterDictionary/positive-words.txt'
negative_path = '/gdrive/My Drive/MasterDictionary/negative-words.txt'
with open(positive_path, 'r', encoding='ISO-8859-1') as file:
            positive_in_file = [line.strip() for line in file.readlines()]
            positive_words.extend(positive_in_file)
with open(negative_path, 'r', encoding='ISO-8859-1') as file:
            negative_in_file = [line.strip() for line in file.readlines()]
            negative_words.extend(negative_in_file)

"""## It reads the Excel file and stores the data in a DataFrame named df."""

df = pd.read_excel('/content/Input.xlsx')

"""# Extracting the text and generating the text file with URL_ID as file name"""

import pandas as pd
import requests
from bs4 import BeautifulSoup

# Function to extract text from a given URL and save it to a text file
def extract_and_save_text(url_id, url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extracting article text
        article_text = ' '.join([p.get_text() for p in soup.find_all('p')])

        # Save the extracted text to a text file with URL_ID as the file name
        with open(f'{url_id}.txt', 'w', encoding='utf-8') as file:
            file.write(article_text)

        print(f"Text extracted and saved for URL_ID {url_id}")

    except Exception as e:
        print(f"Error extracting text for URL_ID {url_id}: {str(e)}")



# Iterate through URLs and extract text
for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    extract_and_save_text(str(url_id), url)  # Convert URL_ID to string to avoid encoding issues

"""## Cleaning the Generated text file by using predefined stopwords and generating the URL_ID_Cleaned text file"""

# Function to extract text from a given URL, clean it using stop words, and save it to a text file
def extract_clean_and_save_text(url_id, url, stop_words):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extracting article text
        article_text = ' '.join([p.get_text() for p in soup.find_all('p')])

        # Tokenize the text
        words = word_tokenize(article_text)

        # Remove stop words
        filtered_words = [word.lower() for word in words if word.lower() not in all_stopwords]

        # Join the filtered words to form the cleaned text
        cleaned_text = ' '.join(filtered_words)

        # Save the cleaned text to a text file with URL_ID as the file name
        with open(f'{url_id}_cleaned.txt', 'w', encoding='utf-8') as file:
            file.write(cleaned_text)

        print(f"Text extracted, cleaned, and saved for URL_ID {url_id}")

    except Exception as e:
        print(f"Error extracting and cleaning text for URL_ID {url_id}: {str(e)}")



# Iterate through URLs and extract, clean, and save text
for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    extract_clean_and_save_text(str(url_id), url, all_stopwords)  # Convert URL_ID to string to avoid encoding issues

""" # creating & displaying dictionary for positive words and negative words"""

# Function to extract text from a given URL, clean it using stop words, and create a dictionary of positive and negative words
def extract_and_create_dictionary(url_id, url, stop_words, positive_words, negative_words, positive_dict, negative_dict):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extracting article text
        article_text = ' '.join([p.get_text() for p in soup.find_all('p')])

        # Tokenize the text
        words = word_tokenize(article_text)

        # Remove stop words
        filtered_words = [word.lower() for word in words if word.lower() not in all_stopwords]

        # Create a dictionary of positive and negative words
        for word in filtered_words:
            if word in positive_words:
                if word in positive_dict:
                    positive_dict[word] += 1
                else:
                    positive_dict[word] = 1
            elif word in negative_words:
                if word in negative_dict:
                    negative_dict[word] += 1
                else:
                    negative_dict[word] = 1

        print(f"Dictionary created for URL_ID {url_id}")

    except Exception as e:
        print(f"Error creating dictionary for URL_ID {url_id}: {str(e)}")


# Define your positive and negative dictionaries
positive_dict = {}
negative_dict = {}

# Iterate through URLs and extract, clean, and create dictionaries
for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    extract_and_create_dictionary(str(url_id), url, all_stopwords, positive_words, negative_words, positive_dict, negative_dict)

# Print the created dictionaries
print("Positive Words Dictionary:")
print(positive_dict)

print("\nNegative Words Dictionary:")
print(negative_dict)

# Function to count syllables in a word
# Function to count syllables in a word
def count_syllables(word):
    vowels = "aeiouy"
    word = word.lower()
    count = 0

    # Remove common suffixes
    if word.endswith("es"):
        word = word[:-2]
    elif word.endswith("ed"):
        word = word[:-2]

    # Count syllables
    if word[0] in vowels:
        count += 1

    for index in range(1, len(word)):
        if word[index] in vowels and word[index - 1] not in vowels:
            count += 1

    if word.endswith("e"):
        count -= 1

    if count == 0:
        count = 1  # Each word has at least one syllable

    return count

# Function to extract text from a given URL, clean it using stop words, and create a dictionary of positive and negative words
def extract_clean_and_calculate_positive_score(url_id, url, stop_words, positive_words, negative_words, positive_dict, negative_dict):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extracting article text
        article_text = ' '.join([p.get_text() for p in soup.find_all('p')])

        # Tokenize the sentences
        sentences = sent_tokenize(article_text)
        # Tokenize the text
        words = word_tokenize(article_text)

        # Remove stop words
        filtered_words = [word.lower() for word in words if word.lower() not in all_stopwords]

        # Create a dictionary of positive and negative words
        for word in filtered_words:
            if word in positive_words:
                if word in positive_dict:
                    positive_dict[word] += 1
                else:
                    positive_dict[word] = 1
            elif word in negative_words:
                if word in negative_dict:
                    negative_dict[word] += 1
                else:
                    negative_dict[word] = 1
        # Calculate Positive Score
        positive_score = sum(positive_dict.get(word, 0) for word in filtered_words)

        # Calculate Negative Score
        negative_score = -1 * sum(negative_dict.get(word, 0) for word in filtered_words)

   # Calculate Polarity Score
        polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)

        # Calculate Subjectivity Score
        subjectivity_score = (positive_score + negative_score) / (len(filtered_words) + 0.000001)
         # Calculate Average Sentence Length
        average_sentence_length = len(words) / len(sentences)

        # Calculate Percentage of Complex Words
        complex_words = [word for word in filtered_words if len(word) > 6]  # You can adjust the threshold for what is considered a complex word
        percentage_of_complex_words = len(complex_words) / len(filtered_words)

        # Calculate Fog Index
        fog_index = 0.4 * (average_sentence_length + percentage_of_complex_words)

        # Calculate Average Number of Words Per Sentence
        average_words_per_sentence = len(words) / len(sentences)

       # Calculate Complex Word Count
        complex_word_count = sum(len(word) > 6 for word in words)
         # Calculate Word Count
        word_count = len(filtered_words)
        # Calculate Syllable Count Per Word

        syllable_count_per_word = sum(count_syllables(word) for word in filtered_words)

    # Calculate Personal Pronouns Count
        personal_pronouns_count = sum(1 for word in filtered_words if re.match(r'\b(?:i|we|my|ours|us)\b', word))
        # Calculate Average Word Length
        average_word_length = sum(len(word) for word in filtered_words) / len(filtered_words)

        print(f"Scores and Readability metrics calculated for URL_ID {url_id}:    Positive Score: {positive_score},   Negative Score: {negative_score},   Polarity Score: {polarity_score:.4f},   Subjectivity Score: {subjectivity_score:.4f},   Average Sentence Length: {average_sentence_length:.2f},   Percentage of Complex Words: {percentage_of_complex_words:.4f},   Fog Index: {fog_index:.2f},   Average Words Per Sentence: {average_words_per_sentence:.2f},   Complex Word Count: {complex_word_count},   Word Count: {word_count},   Syllable Count Per Word: {syllable_count_per_word},   Personal Pronouns Count: {personal_pronouns_count},   Average Word Length: {average_word_length:.2f}")


    except Exception as e:
        print(f"Error extracting, cleaning, and analyzing for URL_ID {url_id}: {str(e)}")

    except Exception as e:
        print(f"Error extracting, cleaning, and analyzing for URL_ID {url_id}: {str(e)}")





# Define your positive and negative dictionaries
positive_dict = {}
negative_dict = {}

# Iterate through URLs and extract, clean, and create dictionaries
for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']
    extract_clean_and_calculate_positive_score(str(url_id), url, all_stopwords, positive_words, negative_words, positive_dict, negative_dict)

# Assuming df is your DataFrame
url_id_to_drop = 'blackassign0056'

# Drop the row with the specified URL_ID
df = df[df['URL_ID'] != url_id_to_drop]

# Now df doesn't contain the row with URL_ID 'blackassign0056'

# Function to count syllables in a word
# Function to count syllables in a word
def count_syllables(word):
    vowels = "aeiouy"
    word = word.lower()
    count = 0

    # Remove common suffixes
    if word.endswith("es"):
        word = word[:-2]
    elif word.endswith("ed"):
        word = word[:-2]

    # Count syllables
    if word[0] in vowels:
        count += 1

    for index in range(1, len(word)):
        if word[index] in vowels and word[index - 1] not in vowels:
            count += 1

    if word.endswith("e"):
        count -= 1

    if count == 0:
        count = 1  # Each word has at least one syllable

    return count

# Function to extract text from a given URL, clean it using stop words, and create a dictionary of positive and negative words
def extract_clean_and_calculate_positive_score(url_id, url, stop_words, positive_words, negative_words, positive_dict, negative_dict):
 try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extracting article text
        article_text = ' '.join([p.get_text() for p in soup.find_all('p')])

        # Tokenize the sentences
        sentences = sent_tokenize(article_text)
        # Tokenize the text
        words = word_tokenize(article_text)

        # Remove stop words
        filtered_words = [word.lower() for word in words if word.lower() not in all_stopwords]

        # Create a dictionary of positive and negative words
        for word in filtered_words:
            if word in positive_words:
                if word in positive_dict:
                    positive_dict[word] += 1
                else:
                    positive_dict[word] = 1
            elif word in negative_words:
                if word in negative_dict:
                    negative_dict[word] += 1
                else:
                    negative_dict[word] = 1
        # Calculate Positive Score
        positive_score = sum(positive_dict.get(word, 0) for word in filtered_words)

        # Calculate Negative Score
        negative_score = -1 * sum(negative_dict.get(word, 0) for word in filtered_words)

   # Calculate Polarity Score
        polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)

        # Calculate Subjectivity Score
        subjectivity_score = (positive_score + negative_score) / (len(filtered_words) + 0.000001)
         # Calculate Average Sentence Length
        average_sentence_length = len(words) / len(sentences)

        # Calculate Percentage of Complex Words
        complex_words = [word for word in filtered_words if len(word) > 6]  # You can adjust the threshold for what is considered a complex word
        percentage_of_complex_words = len(complex_words) / len(filtered_words)

        # Calculate Fog Index
        fog_index = 0.4 * (average_sentence_length + percentage_of_complex_words)

        # Calculate Average Number of Words Per Sentence
        average_words_per_sentence = len(words) / len(sentences)

       # Calculate Complex Word Count
        complex_word_count = sum(len(word) > 6 for word in words)
         # Calculate Word Count
        word_count = len(filtered_words)
        # Calculate Syllable Count Per Word
        syllable_count_per_word = sum(count_syllables(word) for word in filtered_words)

    # Calculate Personal Pronouns Count
        personal_pronouns_count = sum(1 for word in filtered_words if re.match(r'\b(?:i|we|my|ours|us)\b', word))
        # Calculate Average Word Length
        average_word_length = sum(len(word) for word in filtered_words) / len(filtered_words)

        df_result = pd.DataFrame({
    'URL_ID': [url_id],
    'URL':[url],
    'Positive Score': [positive_score],
    'Negative Score': [negative_score],
    'Polarity Score': [polarity_score],
    'Subjectivity Score': [subjectivity_score],
    'Average Sentence Length': [average_sentence_length],
    'Percentage of Complex Words': [percentage_of_complex_words],
    'Fog Index': [fog_index],
    'Average Words Per Sentence': [average_words_per_sentence],
    'Complex Word Count': [complex_word_count],
    'Word Count': [word_count],
    'Syllable Count Per Word': [syllable_count_per_word],
    'Personal Pronouns Count': [personal_pronouns_count],
    'Average Word Length': [average_word_length]})


 except Exception as e:
        print(f"Error extracting, cleaning, and analyzing for URL_ID {url_id}: {str(e)}")
 return df_result


# Append the DataFrame to the predefined Excel file
predefined_file_path = '/content/Output Data Structure.xlsx'  # Replace with the actual path to your predefined Excel file

# Define your positive and negative dictionaries
positive_dict = {}
negative_dict = {}

# Iterate through URLs and extract, clean, and create dictionaries
for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']
    df_result=extract_clean_and_calculate_positive_score(str(url_id), url, all_stopwords, positive_words, negative_words, positive_dict, negative_dict)
with pd.ExcelWriter(predefined_file_path, engine='openpyxl', mode='a') as writer:
      df_result.to_excel(writer, index=False, header=not pd.read_excel(predefined_file_path).shape[0],sheet_name='Sheet2', startrow=1)